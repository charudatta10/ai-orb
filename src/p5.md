# Multi-Agent Blog Writing System with FastMCP and Ollama

Here's a Python implementation of a multi-agent system using FastMCP as the message passing server and Ollama as the LLM client to collaboratively write a blog post.

## System Architecture

1. **MCP Server**: Acts as the central message broker using FastMCP
2. **Agents**:
   - Researcher: Gathers information on the topic
   - Writer: Creates the blog content
   - Editor: Refines and improves the content
   - Publisher: Formats and finalizes the blog post
3. **Ollama Client**: Interface with local LLMs through Ollama

## Implementation

### 1. FastMCP Server Setup

```python
# mcp_server.py
from fastmcp import MCPServer
import asyncio

class BlogMCPServer:
    def __init__(self, host="localhost", port=8000):
        self.server = MCPServer(host=host, port=port)
        self.agents = {}
        
    async def start(self):
        @self.server.on("register")
        async def register_agent(agent_type, agent_id):
            self.agents[agent_id] = agent_type
            print(f"Agent registered: {agent_type} ({agent_id})")
            return {"status": "registered"}
            
        @self.server.on("get_agents")
        async def get_agents():
            return {"agents": self.agents}
            
        await self.server.start()
        
    async def stop(self):
        await self.server.stop()

if __name__ == "__main__":
    server = BlogMCPServer()
    try:
        asyncio.run(server.start())
    except KeyboardInterrupt:
        asyncio.run(server.stop())
```

### 2. Ollama Client

```python
# ollama_client.py
import requests
from typing import Optional

class OllamaClient:
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        
    def generate(self, model: str, prompt: str, system: Optional[str] = None, **kwargs):
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            **kwargs
        }
        if system:
            payload["system"] = system
            
        response = requests.post(f"{self.base_url}/api/generate", json=payload)
        response.raise_for_status()
        return response.json()["response"]
        
    def list_models(self):
        response = requests.get(f"{self.base_url}/api/tags")
        response.raise_for_status()
        return [model["name"] for model in response.json()["models"]]
```

### 3. Base Agent Class

```python
# base_agent.py
from fastmcp import MCPClient
from ollama_client import OllamaClient
import asyncio
import uuid

class BlogAgent:
    def __init__(self, agent_type, mcp_host="localhost", mcp_port=8000, ollama_model="llama3"):
        self.agent_id = f"{agent_type}-{str(uuid.uuid4())[:8]}"
        self.agent_type = agent_type
        self.mcp_client = MCPClient(host=mcp_host, port=mcp_port)
        self.ollama = OllamaClient()
        self.ollama_model = ollama_model
        
    async def connect(self):
        await self.mcp_client.connect()
        await self.mcp_client.send("register", {
            "agent_type": self.agent_type,
            "agent_id": self.agent_id
        })
        
    async def send_message(self, recipient_type, message, data):
        await self.mcp_client.send("message", {
            "sender": self.agent_id,
            "recipient_type": recipient_type,
            "message": message,
            "data": data
        })
        
    async def process_message(self, sender, message, data):
        raise NotImplementedError("Subclasses must implement this method")
        
    async def listen(self):
        @self.mcp_client.on("message")
        async def handle_message(sender, message, data):
            if data.get("recipient_type") == self.agent_type or data.get("recipient_id") == self.agent_id:
                await self.process_message(sender, message, data)
                
    async def generate_text(self, prompt, system=None):
        return self.ollama.generate(self.ollama_model, prompt, system)
        
    async def run(self):
        await self.connect()
        await self.listen()
        while True:
            await asyncio.sleep(1)
```

### 4. Specialized Agents

```python
# agents.py
from base_agent import BlogAgent
import asyncio

class ResearcherAgent(BlogAgent):
    def __init__(self):
        super().__init__("researcher", ollama_model="llama3")
        self.system_prompt = """
        You are a research assistant specializing in gathering and summarizing information.
        Your responses should be factual, well-organized, and include key points.
        """
        
    async def process_message(self, sender, message, data):
        if message == "research_topic":
            topic = data["topic"]
            print(f"Researching topic: {topic}")
            
            research = await self.generate_text(
                f"Provide a comprehensive research on: {topic}",
                self.system_prompt
            )
            
            await self.send_message("writer", "research_results", {
                "topic": topic,
                "research": research
            })

class WriterAgent(BlogAgent):
    def __init__(self):
        super().__init__("writer", ollama_model="llama3")
        self.system_prompt = """
        You are a professional blog writer. Create engaging, well-structured blog posts
        based on the provided research. Use markdown formatting with headings, lists,
        and paragraphs as appropriate.
        """
        
    async def process_message(self, sender, message, data):
        if message == "research_results":
            topic = data["topic"]
            research = data["research"]
            
            print(f"Writing blog post about: {topic}")
            
            blog_post = await self.generate_text(
                f"Write a blog post about {topic} using this research:\n\n{research}",
                self.system_prompt
            )
            
            await self.send_message("editor", "draft_content", {
                "topic": topic,
                "content": blog_post
            })

class EditorAgent(BlogAgent):
    def __init__(self):
        super().__init__("editor", ollama_model="llama3")
        self.system_prompt = """
        You are a professional editor. Improve the provided content by:
        - Fixing grammar and spelling
        - Improving clarity and flow
        - Ensuring consistency
        - Making it more engaging
        Keep the original meaning while enhancing the quality.
        """
        
    async def process_message(self, sender, message, data):
        if message == "draft_content":
            topic = data["topic"]
            content = data["content"]
            
            print(f"Editing blog post about: {topic}")
            
            edited_content = await self.generate_text(
                f"Edit this blog post about {topic}:\n\n{content}",
                self.system_prompt
            )
            
            await self.send_message("publisher", "final_content", {
                "topic": topic,
                "content": edited_content
            })

class PublisherAgent(BlogAgent):
    def __init__(self):
        super().__init__("publisher", ollama_model="llama3")
        self.system_prompt = """
        You are a publisher. Format the content for final publication by:
        - Adding appropriate HTML tags
        - Ensuring proper structure
        - Adding meta information
        - Making it web-ready
        """
        
    async def process_message(self, sender, message, data):
        if message == "final_content":
            topic = data["topic"]
            content = data["content"]
            
            print(f"Publishing blog post about: {topic}")
            
            published_content = await self.generate_text(
                f"Format this blog post for publication:\n\n{content}",
                self.system_prompt
            )
            
            # Save to file
            filename = f"blog_{topic.lower().replace(' ', '_')}.html"
            with open(filename, "w") as f:
                f.write(published_content)
            
            print(f"Blog post published to {filename}")

async def run_agent(agent):
    await agent.run()
```

### 5. Main System Runner

```python
# main.py
from agents import ResearcherAgent, WriterAgent, EditorAgent, PublisherAgent
import asyncio

async def main():
    # Create agents
    researcher = ResearcherAgent()
    writer = WriterAgent()
    editor = EditorAgent()
    publisher = PublisherAgent()
    
    # Start all agents
    tasks = [
        asyncio.create_task(researcher.run()),
        asyncio.create_task(writer.run()),
        asyncio.create_task(editor.run()),
        asyncio.create_task(publisher.run())
    ]
    
    # Give them time to connect
    await asyncio.sleep(2)
    
    # Kick off the process with a research request
    await researcher.send_message("researcher", "research_topic", {
        "topic": "The Future of AI in Healthcare"
    })
    
    # Keep running
    await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main())
```

## How to Use the System

1. **Prerequisites**:
   - Install Ollama and pull the desired models (e.g., `ollama pull llama3`)
   - Install required Python packages: `pip install fastmcp requests`

2. **Start the components**:
   - Run the MCP server: `python mcp_server.py`
   - Run the main system: `python main.py`

3. **Workflow**:
   - The system will automatically:
     1. Research the specified topic
     2. Write a draft blog post
     3. Edit and refine the content
     4. Publish as an HTML file

## Customization Options

1. **Change Models**: Modify the `ollama_model` parameter in each agent to use different models
2. **Add More Agents**: Create new agent classes for specific tasks (e.g., fact-checker, SEO optimizer)
3. **Modify Prompts**: Adjust the system prompts for each agent to change their behavior
4. **Add Web Interface**: Create a Flask/FastAPI frontend to submit topics and view results

This system demonstrates how multiple specialized agents can collaborate through message passing to complete complex tasks like blog writing. Each agent focuses on its specific role while coordinating through the MCP server.